\documentclass[a4paper,masters,en,listoffigures,listoftables]{NMBU}

% If you'd like to use chapters instead of sections, add 'book' to the options, as \documentclass[book,...]{NMBU}. Recall that you now have to use \chapter{} and then \section{}, \subsection{}, ... 

%\usepackage[nohyperlinks]{acronym} % If you need a List of Acronyms, consider using this package

% \usepackage[font=small]{caption} % if you prefer smaller captions
% \captionsetup{width=0.9\linewidth} 

% Word of advice: When you are done with your thesis, change the compiler in Overleaf to XeLaTeX! This will make your fonts and title page look much nicer than in pdfLaTeX. Just go to "Menu", in the "Settings", find "Compiler" and change to XeLaTeX.

% The NMBU style package is designed to help you write the cover and back page automatically. Make sure to include the information below to generate your title. You can set the language you are writing in with "en" for english, "bm" for bokmål, or "nn" for nynorsk.

% A table of contents is automatically included. If you have more than one figure, you need a list of figures by adding "listoffigures" to the documentclass parameters. If you have more than one table, you need a list of tables by adding "listoftables" to the documentclass parameters.

% Additional packages
\usepackage{algorithm}
\usepackage{algpseudocode}  % modern and recommended
\usepackage[intoc]{nomencl}
\makenomenclature

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\v}[1]{\boldsymbol{#1}}

% If you would like to have a bibliography with named authors, organised alphabetically, add "namedauthors" to the parameters of the document class.
\addbibresource{bib.bib} % here you include the bibliography file. 
\graphicspath{{../figures/}}

% If you are using Norwegian dates in your bibliography, add "norwegiandates" to the document class.

% When you finish your thesis, switch the LaTeX compiler from pdfLaTeX to XelaTeX (on Overleaf, "Menu"->"Compiler"). This is for 2 reasons: 1) to produce your front page with the Arial font; 2) XeLaTeX produces better fonts and (usually) smaller PDFs. Every kilobyte of space we save helps reduce CO2 emissions :)

% Good luck with your thesis! If you get in trouble with LaTeX stuff, write to me: leonardo.rydin@gmail.com

% In your .tex file, you need to include the following information to produce the first page of your thesis

\title{Matrix-Free Randomized Sketching for Column-Norm Estimation of Discretized PDE Operator}
\author{Elias Jegervatn Severinsen}
\thesisyear{2026}
\credits{30}
\faculty{Faculty of Science and Technology}
\studyprogramme{Data Science}
\supervisor{Ole Løseth Elvetun, Jonas Kusch} % Include the name of the supervisor or supervisors (this is added to the metadata of the PDF)
%\engtitle{} % Only if you write a thesis in a Scandinavian language

% You should also use the "\abstract" and "\sammendrag" to produce those. If you are writing in a Scandinavian language, the sammendrag will appear first.
\abstract{This thesis uses randomized linear algebra for estimation of a discrete PDE operator.}
\sammendrag{Denne masteroppgaven bruker randomisert lineær algebra for mastrise-fri estimering av PDE-operatorer.}
%\acknowledgements{} You can add acknowledgements here. In Scandinavian languages, use "\foreword{}" (or if in English you prefer Foreword to Acknowledgements). The foreword comes before the abstract.

\begin{document}

% If you need a List of Acronyms, place them before the \startthesis command. You will need "\usepackage[nohyperlinks]{acronym}", see line 4.

%\section*{List of Symbols}
% \begin{acronym}
%   \acro{TSO}{Transmission System Operator}
%   \acro{LSTM}{Long Short-Term Memory}
%\end{acronym}

\section*{List of Symbols}

\begin{tabular}{ll}
    $\Omega$ & Problem domain \\
    $\partial\Omega$ & Domain boundary \\
    $A$ & Stiffness matrix \\
    $M$ & (Domain) mass matrix \\
    $M_{ds}$ & Boundary mass matrix \\
    $K$ & Discrete forward operator \\
    $K^*$ & Adjoint of the discrete forward operator \\
    $W$ & Weight matrix in the regularization \\
    $\lambda$ & Regularization parameter \\
    $\Xi$ & Random test matrix
\end{tabular}


\startthesis % This command will separate the Roman-numbered pages above and the Arabic-numbered pages below as well as create the table of contents, list of figures, list of tables, and potentially list of acronyms

\section{Introduction}
%How do you model the spread of an infectious virus, the flow of traffic during morning rush hour, the airflow over an airplane, the interactions between fundamental particles which make up all matter, or the space and time of the universe? Despite the apparent diversity, these phenomena all describe how quantities change across time and space. Partial differential equations (PDEs) arise naturally, and are central in the modelling of such systems.

%For instance, an epidemiologist can use the SIR-model to predict the spread of an infectious virus through a population \cite{kroger_analytical_2020}. A traffic engineer may wish to investigate how an initial congestion on an intersection spreads through the road network by using the method of kinematic waves \cite{lighthill_kinematic_1955}. An airplane company can model airflow and turbulence of an aircraft by the use of the Navier-Stokes equations \cite{j_m_mathematical_1948}. A particle scientist can model particle interactions by the use of Schrödinger equations \cite{schrodinger_quantisierung_1926}. And a astro astrophysicist can use Einsteins field equations to model the evolution of the universe, the death of a star, or the properties of a black hole \cite{einstein_grundlage_1916}. These are only some examples of PDEs and their applications.

Partial differential equations (PDEs) are essential in modern modelling. They have applications in finance, physics, medicine, and biology, to name a few. The discretization of PDEs, which enables numerical solutions of such equations, can produce linear systems of equations with a large number of unknowns. For instance, in modern applications, it is not uncommon with billions of unknowns. It is therefore essential with efficient algorithms to handle large scale linear systems of equations.

In this thesis, we study the class of elliptic PDEs, which typically occur in time-independent problems. This class of PDEs show up in steady-state problems, stochastic processes, and the diffusion and flow a quantity in a region of space \cite{pavliotisStochasticProcessesApplications2014, evansPartialDifferentialEquations2010}.

In particular, we study the inverse problem of the form
\begin{equation*}
    \min_{f} \left\| \mathcal{K} f - y \right\|_{L^2(\partial\Omega)}^2
    + \lambda^2 \left\| W f \right\|_{L^2(\Omega)}^2
\end{equation*}
subject to
\begin{eqnarray*}
    \begin{aligned}
    -\nabla \cdot \sigma\nabla u + ku &= f \quad \text{in } \Omega, \\
    \frac{\partial u}{\partial \v n} &= 0 \quad \text{on } \partial\Omega.
    \end{aligned}
\end{eqnarray*}
This problem has numerous applications, such as inverse imaging, crack determination, and electromagnetic brain mapping \cite{elvetun_regularization_2020}. This problem is a source identification task: we are given observed data $y$, and seek to find the original source term $f$ such that $\mathcal K f = y$, where $\mathcal K: f \to u|_{\partial\Omega}$ is the mapping from the source $f$ to the output on the boundary of $\Omega$, $u|_{\partial\Omega}$.

However, source identification problems of this form are ill-posed, as the operator $\mathcal K$ has a large null space. Consequently, solutions are not uniquely determined by the data. Moreover, small perturbations the observed data may result in large variations in the reconstructed source. Due to these challenges, typical techniques such as Tikhonov regularization fail to recover a meaningful source term.

%[Large null space, ill-posed]
%[Vis til klassisk Tikhonov]
%However, the methods presented in this thesis are not limitied to this particular equation, and may be applied to elliptic PDEs in general.]


\clearpage
\section{Mathematical background}

\nomenclature{$\Omega$}{Computational domain}
\nomenclature{$\partial\Omega$}{Boundary of the domain}
\nomenclature{$A$}{Stiffness matrix}
\nomenclature{$M$}{Mass matrix}

\subsection{The forward operator}
\input{figures/forward_operator.tex}

Let $\Omega \subset \mathbb R^2$ be a bounded domain with boundary $\partial \Omega$. I consider the following second-oder elliptic boundary value problem (BVP):
\begin{eqnarray}\label{eq:pde}
    \begin{aligned}
    -\nabla \cdot \sigma\nabla u + ku &= f \quad \text{in } \Omega, \\
    \frac{\partial u}{\partial \v n} &= 0 \quad \text{on } \partial\Omega,
    \end{aligned}
\end{eqnarray}
where $k > 0$ and $\sigma, f \in H^1(\Omega)$ (TODO: hva er $\sigma$). This is a diffusion-reaction equation with Neumann boundary conditions. Equation~\eqref{eq:pde} defines a mapping $\mathcal S: H^1(\Omega) \rightarrow H^1(\Omega)$. If $f \in H^1(\Omega)$ is the source function (the input), then Equation~\eqref{eq:pde} maps $f$ to $\mathcal S f = u$ (the output). 

Introduce the trace operator $\mathcal T: H^1(\Omega) \rightarrow H^1(\partial\Omega)$, defined by
\begin{equation}\label{eq:trace}
    \mathcal T: u \mapsto g = u|_{\partial\Omega}.
\end{equation}
$\mathcal T u$ represents the observed values of $u$ on the boundary of $\Omega$. Operator~\eqref{eq:pde} and \eqref{eq:trace} defines the forward operator $\mathcal K: H^1(\Omega) \rightarrow H^1(\partial\Omega)$, 
\begin{equation}
    \mathcal K = \mathcal T \circ \mathcal S,
\end{equation}
which is a mapping from the source function $f$ to the observed output $g = \mathcal K f$ on the boundary $\partial \Omega$. Figure~\ref{fig:forward_operator} illustrates the components of the forward operator, which defines the inverse problem: given observed boundary function $g$, we seek a source function $f$ such that $\mathcal{K} f = g$. In order to solve such a problem computationally, the problem must be discretized. This is done through the finite element method, described in the following sections.


\subsection{The variational formulation}
To derive a finite element formulation of Eq.~\eqref{eq:pde} and the forward operator $\mathcal K$, I first reformulate the boundary value problem in its variational (weak) from. Multiplying the differential equation by a test function $v \in H^1(\Omega)$, integrating over $\Omega$, and applying Green's identity and the Neumann boundary conditions, results in the following variational formulation: find $u \in H^1(\Omega)$ such that
\begin{equation}\label{eq:variational_formulation}
    \int_{\Omega} (\sigma \nabla u \cdot \nabla v + k u v) \, dx = \int_{\Omega} f v \space \, dx,
    \text{ for all } v \in H^1(\Omega).
\end{equation}
For further details, see Section~4 in Larson and Bengzon \cite{larson_finite_2013}.


\subsection{The finite element method}
\input{figures/triangulation.tex}
Let $\mathcal M$ be a triangulation of $\Omega$ made up of $N$ nodes, with $N_b$ boundary nodes, see Fig.~\ref{fig:triangulation} as an example. The finite element space is the finite-dimensional $V_h \subset H^1(\Omega)$,
\begin{equation}\label{eq:fem_space}
    V_h = \{v \in C^0(\Omega) : v|_M \text{ is linear for all } M \in \mathcal M \}.
\end{equation}
This is the space of all continuous functions which are piecewise linear on $\mathcal{M}$. Let $\Phi = \{\phi_1, \dots, \phi_N\}$ be a basis $V_h$, i.e., $\operatorname{span}(\Phi) = V_h$. The basis functions $\phi_1, \dots, \phi_N$ are typically hat functions (see \parencite[p.~51]{larson_finite_2013}). For each node making up the mesh $\mathcal{M}$, there is a corresponding basis function. Since the mesh $\mathcal{M}$ consists of $N$ nodes and $N_b$ boundary nodes, then the basis set $\Phi$ must contain $N$ basis function, with $N_b$ being located on the mesh boundary.

By replacing $H^1(\Omega)$ with $V_h$ in Eq.~\eqref{eq:variational_formulation}, I get the finite element method: find $u_h \in V_h$ such that
 \begin{equation}\label{eq:fem_formulation}
    \int_{\Omega} (\sigma \nabla u_h \cdot \nabla v + k u_h v) \, dx = \int_{\Omega} f v \space \, dx,
    \text{ for all } v \in V_h.
\end{equation}


\subsection{The discrete inverse problem}
In order to represent the forward operator $\mathcal{K} = \mathcal{T} \circ \mathcal{S}$ in a discrete form which can be represented computationally, both the PDE operator $\mathcal S$ and the trace $\mathcal T$ must be discretized, i.e., their matrix representations $S$ and $T$ must be found. Using finite element representations of $f$, $u$ and $g$, the problem $\mathcal{K}f = g$ can be expressed in terms of a system of linear equations.

Discretizing the PDE operator $\mathcal{S}$, which represents the finite element formulation in Eq.~\eqref{eq:fem_formulation}, is done by replacing the source function $f$ by its finite element approximation $f_h \in V_h$. Since both $u_h$ and $f_h$ are functions of the subspace $V_h$, they may be written as a linear combinations of the basis functions $\phi_1, \dots, \phi_N$,
\begin{equation}\label{eq:uh_and_fh}
    u_h = \sum_{j=1}^{N} u_j \phi_j, \quad
    f_h = \sum_{j=1}^{N} x_j \phi_j.
\end{equation}
where $\{ \phi_1, \dots, \phi_N \}$ is a basis for $V_h$. Let $\v x = [x_1, \, \dots, \, x_N]$ and $\v u = [u_1, \, \dots, \, u_N]$ be the coefficient vectors for $f_h$ and $u_h$, respectively.
By inserting Eq.~\eqref{eq:uh_and_fh} into the finite element formulation in Eq.~\eqref{eq:fem_formulation}, following system of linear equations (matrix-vector equation) is obtained:
\begin{equation}\label{eq:matrix_vector_equation}
    A \v x = M \v u.
\end{equation}
$A$ is known as the stiffness matrix and $M$ the mass matrix. Solving Eq.~\eqref{eq:matrix_vector_equation} for $\v x$, we get that $\v x = A^{-1} M \v u$, so the discretization $S\in \mathbb{R}^{N \times N}$ of the PDE operator $\mathcal S$ is therefore
\begin{equation}\label{eq:discrete_S}
    S = A^{-1}M.
\end{equation}
Note that $S$ maps the coefficients $\v x$ of $f_h$ to the coefficients $\v u$ of $u_h$ in their finite element representations given in Eq.~\eqref{eq:uh_and_fh}.

The discretization $T: \mathbb{R}^N \rightarrow \mathbb{R}^{N_b}$ of $\mathcal T$ is a $N_b \times N$ matrix which extracts the boundary coefficients of a function. Each row has one element equal to 1, corresponding to the boundary node index, and 0s elsewhere. Next, using the discrete operators $S$ and $T$, we get that the discretized formulation of the forward operator $\mathcal K = \mathcal T \circ \mathcal S$ is
\begin{equation}\label{eq:discrete_K}
    K = T S = T A^{-1} M \in \mathbb R^{N_b \times N}.
\end{equation}

By denoting the boundary coefficients of $\v u$ by $\v y$, i.e., $T \v u = \v y$, we can formulate $\mathcal{K}f = u|_{\partial\Omega}$ in its discretized version:
\begin{equation}\label{eq:inverse_problem}
    K \v x = \v y.
\end{equation}
Equation~\eqref{eq:inverse_problem} represents the discrete inverse problem: given observed boundary data $\v y$, we seek the coefficient vector $\v x$ of the source term $f_h$.

%In practice, this system is often **ill-posed** due to the large null space of $K$ and the accumulation of numerical errors in $A^{-1}$. As a result, naive solutions of Eq.~\eqref{eq:inverse_problem} are highly sensitive to noise in $\v y$, motivating the need for regularization techniques. In the following section, we discuss classical Tikhonov regularization and introduce the weighted approach that will be explored in this thesis.


\subsection{Tikhonov regularization}
Theory on Tikhonov regularization,
\begin{equation}
    \min_{x \in \mathbb R^n} \| A x - b \|^2 + \lambda^2 \| x \|^2
\end{equation}


\subsection{Randomized SVD}
The rSVD is a technique used to compute a rank-$k$ approximation of an $m\times n$ matrix without computing the SVD. The core idea is to use random sampling to find a low-rank matrix $Q$ which approximates the range of $A$, project $A$ onto $\operatorname{span}(Q)$, and compute the SVD of the projection. The projection $B=Q^TA$ will, with high probability, capture the dominant actions of $A$, and the SVD of $B$ will yield a good approximation \cite{halko_finding_2011}. This procedure is summarized in Algorithm \ref{alg:rsvd}. The construction of an orthonormal basis $Q$ of $Y$ is done through a QR factorization. Note that the approximation $A\approx QQ^T A$ and the approximate factorization $A\approx U\Sigma V^T$ produced by rSVD have the same error. This can be seen by observing that $QQ^TA = QB = Q\tilde U\Sigma V^T$, and setting $U=Q\tilde U$.
\begin{algorithm}[!htbp]
\caption{rSVD}
\label{alg:rsvd}
\begin{algorithmic}[1]
    \Require $A\in\mathbb R^{m\times n}$, target rank $k$, oversampling parameter $p$.
    \Ensure Approximate rank-$k$ factorization $U\Sigma V^T$.
    \State Generate a random $n\times(k+p)$ test matrix $\Omega$.
    \State Compute $Y = A\Omega$.
    \State Form an orthonormal basis $Q$ of $Y$.
    \State Set $B = Q^TA$.
    \State Compute the SVD $B = \tilde U\Sigma V^T$.
    \State Set $U = Q\tilde U$. 
\end{algorithmic}
\end{algorithm}


\subsection{Tips}
When you are closely following a book to explain something, which is often the case in a theory section, you can write at the start of the section you are about to introduce: ``This section follows closely the reference book by Sumiyoshi Abe and Yuko Okamoto, \emph{Nonextensive Statistical Mechanics and Its Applications}~\parencite[p.~32]{Abe2001nonextensive}.''

A Master's~\cite{LastName2045norwegian} or a PhD thesis~\cite{Temult2038binding} should include the name of the university wherein it was written, as well as the year. Moreover, it should include a URL to the work, when available.

\clearpage
\section{Methods}
\subsection{Matrix-free randomized SVD}
\input{figures/rsvd.tex}
Let $K$ be the $N_b \times N$ discrete forward operator given in Eq. (eqref). We are interested in a rank-$k$ approximation $K_k$ of the matrix $K$. By the Eckart–Young–Mirsky theorem, the optimal rank-$k$ approximation of $K$ is the truncated SVD obtained by keeping the $k$ first singular values, and their corresponding singular values. The randomized SVD provides an efficient alternative, by computing an approximate rank-$k$ factorization $K \approx U_k \Sigma_k V_k$, which also approximates the optimal truncated rank-$k$ SVD decomposition. We will now present a matrix-free variation of the rSVD algorithm, suited for problems where the matrix $K$ is not explicitly known.

Like the rSVD, this matrix-free formulation consists of two stages. In the first stage, an approximate basis $Q$ of $K$ is constructed through random sampling of the column space of $K$. In the second stage, the matrix $K$ is projected onto the subspace $\operatorname{span}(Q)$, and a SVD factorization is computed of the projected matrix.

In the first stage, we apply $K$ on $k$ random vectors $\psi_1, \psi_2, \dots, \psi_l$ drawn from some distribution, typically the normal distribution. This yields the random samples $y_1, y_2, \dots, y_k$, where $y_i = K \psi_i$. If we gather the samples and random vectors into matrices $Y = [y_i]_{i=1}^k$ and $\Psi = [\psi_i]_{i}$, we can express $Y$ as
\begin{equation}
    Y = K \Psi.
\end{equation}
(something about how $y_i$ is most likely to line up with the first singular vector). Geometrically, we want the $i$'th sample $y_i$ to approximately point in the direction of the the $i$ right singular vector $v_i$. To remove contributions of the previous right singular vectors $v_{i-1}, v_{i-2}, \dots, v_{1}$ on $y_i$, we must orthogonalize the random samples. A QR-factorization
\begin{equation}
    QR = Y
\end{equation}
achieves such an orthogonalization, obtaining the matrix $Q$ which contains the orthogonalized column-vectors $q_1, q_2, \dots, q_k$. This concludes the first step, where we have obtained an approximation of the column space of $K$:
\begin{equation}
    \operatorname{span}\{q_1, q_2, \dots, q_k \} \approx \operatorname{span}(K)
\end{equation}

For the second stage, we project $K$ down to the subspace $\operatorname{span}(Q)$. This is typically done by computing $B = Q^T K$, however, since we do not know $K$ explicitly, we instead compute $B^* = K^* Q$, and then $B$ can be found with as $B = (B^*)^*$. To do this, we do as follows: for $q_1, q_2, \dots, q_k$, compute $\tilde q_i = M_{ds}^{-1} q_i$, and set $\tilde b = K^* \tilde q$, and let
\begin{equation}
    b_i = \tilde b_i^T M_{dx} = (K^* \tilde q_i)^T M_{dx}.
\end{equation}
From the definition of $K^*$ we know that
\begin{equation}
    (K^* z)^T M_{dx} = z^T M_{ds}K,
\end{equation}
for all $z$. Hence, setting $z = \tilde q_i$, we have that $(K^* \tilde q_i)^T M_{dx} = \tilde q_i^T M_{ds} K$, and from Eq~\eqref{}, we get that
\begin{equation}
\begin{aligned}
    b_i &= \tilde q_i^T M_{ds} K \\
    &= (M_{ds}^{-1} q_i)^T M_{ds} K \\
    &= q_i^T M_{ds}^{-1} M_{ds} K \\
    &= q_i^T K
\end{aligned}
\end{equation}
where used the fact that $M_{ds}$ is symmetric to get that $(M^{-1})^T = M^{-1}$. By repeating this for $i=1,2,\dots,k$, we can construct the $k \times N$ matrix
\begin{equation}
    B = \begin{bmatrix} b_1 \\ \vdots \\ b_p \end{bmatrix}
\end{equation}
and since $b_i = q_i^T K$, we conclude that
\begin{equation}
    B = Q^T K.
\end{equation}
We have therefore been able to project $K$ onto $Q$ by solving $k$ adjoint problems $\tilde b_i = K^* \tilde q_i$. As the final part of the second stage, we take the SVD of $B$,
\begin{equation}
    B = \tilde U_k \Sigma_k V_k^T
\end{equation}
and by letting $U_k = Q \tilde U_k$, we get an approximate SVD decomposition of the form
\begin{equation}
    K \approx U_k \Sigma_k V_k^T.
\end{equation}
By noting that $U_k \Sigma_k V_k^T = Q \tilde U_k \Sigma_k V_k^T = Q B = Q Q^T K$, and that $\operatorname{rank}(Q) = k$, we know that the factorization in Eq~\eqref{} is a rank-$k$ approximation of $K$. The procedure is summarized in Algorithm~\ref{alg:discrete_rsvd}.


\begin{algorithm}[!thbp]
\caption{Discrete Operator rSVD Approximation of $K$}
\label{alg:discrete_rsvd}
\begin{algorithmic}[1]
    \Require Forward operator $K: \mathbb{R}^N \to \mathbb{R}^{N_b}$, adjoint operator $K^*: \mathbb{R}^{N_b} \to \mathbb{R}^N$, mass matrices $M_{dx}, M_{ds}$, target rank $p$
    \Ensure Approximate rank-$p$ factorization $K \approx U \Sigma V^T$
    
    \State \textbf{Range sketch:}
    \For{$i = 1, \dots, p$}
        \State Draw a random vector $\v \xi_i \in \mathbb{R}^N$
        \State Apply the forward operator: $\v y_i = K \v \xi_i \in \mathbb{R}^{N_b}$
    \EndFor

    \State Form the matrix $Y = [\v y_1, \dots, \v y_p] \in \mathbb{R}^{N_b \times p}$ \Comment{Equivalent to $Y = K \Xi$}
    \State Compute an orthonormal basis: $Q = \text{orth}(Y)$

    \State \textbf{Projection onto the basis:}
    \For{$i = 1, \dots, p$}
        \State Let $\v q_i$ be the $i$-th column of $Q$
        \State Compute $\tilde{\v{q}}_i = M_{ds}^{-1} \v{q}_i$
        \State Set $\v{b}_{\text{adj},i} = K^* \tilde{\v{q}}_i$
        \State Let $\v{b}_i = b_{\text{adj},i}^T M_{dx}$ \Comment{$i$-th row of $B$}
    \EndFor
    \State Form the projected matrix $B = \begin{bmatrix} b_1 \\ \vdots \\ b_p \end{bmatrix} \in \mathbb{R}^{p \times N}$ \Comment{Equivalent to $B = Q^T Y$}

    \State Compute the SVD: $B = \tilde U \Sigma V^T$
    \State Set $U = Q \tilde U$
    
\end{algorithmic}
\end{algorithm}

\clearpage
\subsection{Dynamic low-rank solver}
Let $X = \operatorname{matrix}(x)$ and $X = U_x \Sigma_x V_x^T$ be the SVD of $X$ and let $\Phi(x)$ be some differentiable cost function of $x$, and $D = \operatorname{matrix}(\nabla \Phi(x))$ its matrix equivalent. We wish to update $X^{(k)}$ according to
$$
X^{(k+1)} = X^{(k)}-\alpha D
$$
Inserting the SVD of $X^{(k)}$ at step $k$, $X^{(k)} = U_x^{(k)} \Sigma_x^{(k)} (V_x^{(k)})^T$, we get that
$$
U_x^{(k+1)} \Sigma_x^{(k+1)} (V_x^{(k+1)})^T = U_x^{(k)} \Sigma_x^{(k)} (V_x^{(k)})^T - \alpha D. \quad (1)
$$
If we right-multiply by $V_x^{(k)}$ and let $W^{(k)} = U_x^{(k)} \Sigma_x^{(k)}$, we get
$$
W^{(k+1)}(V_x^{(k+1)})^T(V_x^{(k)}) = W^{(k)} - \alpha D V_x^{(k)}.
$$
The approximation $(V_x^{(k+1)})^T(V_x^{(k)}) \approx I$ suggests the following updating scheme:
$$
W^{(k+1)} = W^{(k)} - \alpha D V_x^{(k)}.
$$
If we transpose  Equation $(1)$, right-multiply by $U_x^{(k)}$ and denote $L^{(k)} = V_x^{(k)} (\Sigma_x^{(k)})^T$, we get a similar updating scheme for $L^{(k)}$,
$$
L^{(k+1)} = L^{(k)} - \alpha D^T U_x^{(k)}.
$$
We now have update schemes for $W^{(k)} = U_x^{(k)} \Sigma_x^{(k)}$ and $L^{(k)} = V_x^{(k)} (\Sigma_x^{(k)})^T$. Further, we expand and orthogonalize the basis' $U^{(k)}$ and $V^{(k)}$ with the updated weighted basis' $W^{(k+1)}$ and $L^{(k+1)}$, respectively, in the following way:
$$
\begin{aligned}
\hat U &= \operatorname{orth}\begin{bmatrix} U_x^{(k)} & W^{(k+1)}\end{bmatrix}, \\
\hat V &= \operatorname{orth}\begin{bmatrix} V_x^{(k)} & L^{(k+1)}\end{bmatrix}.
\end{aligned}
$$
Currently, we have obtained an approximation $X \approx \hat U \Sigma_x \hat V^T$. In terms of $\Sigma_x$, we have
$$
\Sigma_x \approx \hat U^T X \hat V.
$$
Substituting $X = U_x \Sigma_x V_x^T$ gives the approximation $\hat \Sigma^{(k)}$ of $\Sigma_x$,
$$
\Sigma_x \approx \hat \Sigma^{(k)} =(\hat U^T U_x)\Sigma_x(V_x^T \hat V). \quad (2)
$$
Now, if we write the gradient descent scheme $X^{(k+1)} = X^{(k)} - \alpha D$ as
$$
\hat U^T X^{(k+1)} \hat V = \hat U^TX^{(k)}\hat V - \alpha \hat U^T D \hat V,
$$
and use Equation $(2)$, we get the following updating scheme for $\hat \Sigma^{(k)}$,
$$
\hat \Sigma^{(k+1)} = \hat \Sigma^{(k)} - \alpha \hat U^T D \hat V.
$$
The final step is then to truncate these updated matrices to obtain
$$
U_x^{(k+1)}, \Sigma_x^{(k+1)}, V_x^{(k+1)} = \operatorname{truncate}(\hat U, \hat \Sigma, \hat V)
$$
and finally, set $X^{(k+1)} = U_x^{(k+1)} \Sigma_x^{(k+1)} (V_x^{(k+1)})^T$.


\clearpage
\section{Literature Review}


\clearpage
\section{Methodology}
In the methodology, you might include mentions of various \texttt{python} (or other programming languages) packages.


\clearpage
\section{Results}


\clearpage
\section{Discussion}

\clearpage
\section{Conclusion}

% Note that the appendices should come after the bibliography, not before.
\clearpage
\references % This command prints out your bibliography

\clearpage
\appendices % This command is needed to start the appendix section and number the figures and tables accordingly. You should still use "\section"s to make your appendices.

\end{document}
